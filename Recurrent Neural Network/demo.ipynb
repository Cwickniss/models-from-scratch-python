{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    A simple RNN implementation.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size of the input vectors.\n",
    "        hidden_size (int): The size of the hidden layer.\n",
    "        output_size (int): The size of the output vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, init_method=\"random\"):\n",
    "        self.weights_ih, self.weights_hh, self.weights_ho = self.initialize_weights(input_size, hidden_size, output_size, init_method)\n",
    "        self.bias_h = np.zeros((1, hidden_size))\n",
    "        self.bias_o = np.zeros((1, output_size))\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def initialize_weights(self, input_size, hidden_size, output_size, method):\n",
    "        if method == \"random\":\n",
    "            weights_ih = np.random.randn(input_size, hidden_size) * 0.01\n",
    "            weights_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "            weights_ho = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        elif method == \"xavier\":\n",
    "            weights_ih = np.random.randn(input_size, hidden_size) / np.sqrt(input_size / 2)\n",
    "            weights_hh = np.random.randn(hidden_size, hidden_size) / np.sqrt(hidden_size / 2)\n",
    "            weights_ho = np.random.randn(hidden_size, output_size) / np.sqrt(hidden_size / 2)\n",
    "        elif method == \"he\":\n",
    "            weights_ih = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "            weights_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2 / hidden_size)\n",
    "            weights_ho = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid initialization method\")\n",
    "        return weights_ih, weights_hh, weights_ho\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the RNN.\n",
    "        \n",
    "        Args:\n",
    "            inputs (list): A list of input vectors.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The output vector.\n",
    "        \"\"\"\n",
    "        h = np.zeros((1, self.hidden_size))\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: h}\n",
    "\n",
    "        for i, x in enumerate(inputs):\n",
    "            x = x.reshape(1, -1)  # Ensure x is a row vector\n",
    "            h = np.tanh(np.dot(x, self.weights_ih) + np.dot(h, self.weights_hh) + self.bias_h)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        y = np.dot(h, self.weights_ho) + self.bias_o\n",
    "        self.last_outputs = y\n",
    "        return y\n",
    "\n",
    "    def backprop(self, d_y, learning_rate, clip_value=1):\n",
    "        \"\"\"\n",
    "        Perform backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            d_y (np.ndarray): The gradient of the loss with respect to the output.\n",
    "            learning_rate (float): The learning rate.\n",
    "        \"\"\"\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        d_y_pred = (self.last_outputs - d_y) / d_y.size\n",
    "        d_Whh = np.zeros_like(self.weights_hh)\n",
    "        d_Wxh = np.zeros_like(self.weights_ih)\n",
    "        d_Why = np.zeros_like(self.weights_ho)\n",
    "        d_bh = np.zeros_like(self.bias_h)\n",
    "        d_by = np.zeros_like(self.bias_o)\n",
    "        d_h = np.dot(d_y_pred, self.weights_ho.T)\n",
    "\n",
    "        for t in reversed(range(1, n + 1)):\n",
    "            d_h_raw = (1 - self.last_hs[t] ** 2) * d_h\n",
    "            d_bh += d_h_raw\n",
    "            d_Whh += np.dot(self.last_hs[t - 1].T, d_h_raw)\n",
    "            d_Wxh += np.dot(self.last_inputs[t - 1].reshape(1, -1).T, d_h_raw)\n",
    "            d_h = np.dot(d_h_raw, self.weights_hh.T)\n",
    "\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -clip_value, clip_value, out=d)\n",
    "            \n",
    "        self.weights_ih -= learning_rate * d_Wxh\n",
    "        self.weights_hh -= learning_rate * d_Whh\n",
    "        self.weights_ho -= learning_rate * d_Why\n",
    "        self.bias_h -= learning_rate * d_bh\n",
    "        self.bias_o -= learning_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        patience (int): Number of epochs to wait before stopping the training.\n",
    "        verbose (bool): If True, prints a message for each epoch where the loss\n",
    "                        does not improve.\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Determines if the model should stop training.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): The loss of the model on the validation set.\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    \"\"\"\n",
    "    A class to train an RNN model.\n",
    "\n",
    "    Args:\n",
    "        model (RNN): The RNN model to train.\n",
    "        loss_func (str): The loss function to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_func='mse'):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the loss.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: numpy array\n",
    "            The true output\n",
    "        y_pred: numpy array\n",
    "            The predicted output\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        if self.loss_func == 'mse':\n",
    "            return np.mean((y_pred - y_true)**2)\n",
    "        \n",
    "        elif self.loss_func == 'log_loss':\n",
    "            return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
    "        \n",
    "        elif self.loss_func == 'categorical_crossentropy':\n",
    "            return -np.mean(y_true*np.log(y_pred))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid loss function')\n",
    "\n",
    "    def train(self, train_data, train_labels, val_data, val_labels, epochs, learning_rate, early_stopping=True, patience=10, clip_value=1):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Args:\n",
    "            train_data (list): A list of training data.\n",
    "            train_labels (list): A list of training labels.\n",
    "            val_data (list): A list of validation data.\n",
    "            val_labels (list): A list of validation labels.\n",
    "            epochs (int): The number of epochs to train for.\n",
    "            learning_rate (float): The learning rate.\n",
    "            early_stopping (bool): Whether to use early stopping.\n",
    "            patience (int): The number of epochs to wait before stopping the training.\n",
    "        \"\"\"\n",
    "        if early_stopping:\n",
    "            early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        for epoch in range(epochs):\n",
    "            for X_train, y_train in zip(train_data, train_labels):\n",
    "                outputs = self.model.forward(X_train)\n",
    "                self.model.backprop(y_train, learning_rate, clip_value)\n",
    "                train_loss = self.calculate_loss(y_train, outputs)\n",
    "                self.train_loss.append(train_loss)\n",
    "\n",
    "            val_loss_epoch = []\n",
    "            for X_val, y_val in zip(val_data, val_labels):\n",
    "                val_outputs = self.model.forward(X_val)\n",
    "                val_loss = self.calculate_loss(y_val, val_outputs)\n",
    "                val_loss_epoch.append(val_loss)\n",
    "\n",
    "            val_loss = np.mean(val_loss_epoch)\n",
    "            self.val_loss.append(val_loss)\n",
    "\n",
    "            if early_stopping:\n",
    "                early_stopping(val_loss)\n",
    "\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping at epoch {epoch} | Best validation loss = {-early_stopping.best_score:.3f}\")\n",
    "                    break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}: Train loss = {train_loss:.4f}, Validation loss = {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset:\n",
    "    def __init__(self, url, look_back=1, train_size=0.67):\n",
    "        self.url = url\n",
    "        self.look_back = look_back\n",
    "        self.train_size = train_size\n",
    "\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.url, usecols=[1])\n",
    "        df = self.MinMaxScaler(df.values)  # Convert DataFrame to numpy array\n",
    "        train_size = int(len(df) * self.train_size)\n",
    "        train, test = df[0:train_size,:], df[train_size:len(df),:]\n",
    "        return train, test\n",
    "    \n",
    "    def MinMaxScaler(self, data):\n",
    "        numerator = data - np.min(data, 0)\n",
    "        denominator = np.max(data, 0) - np.min(data, 0)\n",
    "        return numerator / (denominator + 1e-7)\n",
    "\n",
    "    def create_dataset(self, dataset):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-self.look_back-1):\n",
    "            a = dataset[i:(i+self.look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + self.look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    def get_train_test(self):\n",
    "        train, test = self.load_data()\n",
    "        trainX, trainY = self.create_dataset(train)\n",
    "        testX, testY = self.create_dataset(test)\n",
    "        return trainX, trainY, testX, testY\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
    "dataset = TimeSeriesDataset(url, look_back=1)\n",
    "trainX, trainY, testX, testY = dataset.get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss = 0.0014, Validation loss = 0.0330\n",
      "Epoch 10: Train loss = 0.0047, Validation loss = 0.0102\n",
      "Early stopping at epoch 20 | Best validation loss = 0.010\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "look_back = 1\n",
    "\n",
    "# Create and train the RNN\n",
    "rnn = RNN(look_back, 256, 1, init_method='xavier')\n",
    "trainer = RNNTrainer(rnn, 'mse')\n",
    "trainer.train(trainX, trainY, testX, testY, epochs=100, learning_rate=0.01, early_stopping=True, patience=10, clip_value=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
